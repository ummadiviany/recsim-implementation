{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSim: Overview.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ummadiviany/recsim-implementation/blob/master/RecSim_Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3hSuGSmmWWy"
      },
      "source": [
        " Copyright 2019 The RecSim Authors.\n",
        "\n",
        " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        " you may not use this file except in compliance with the License.\n",
        " You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        " Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CkMTOmHpHKE8"
      },
      "source": [
        "# Running RecSim\n",
        "In this Colab we explore how to train and evaulate an agent within RecSim using the provided environments and clarify some basic concepts along the way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VX8egcjA2A3e"
      },
      "source": [
        "\n",
        "# RecSim at a Glance\n",
        "RecSim is a configurable platform for simulating a recommendation system environment in which a recommender agent interacts with a corpus of documents (or recommendable items) and a set of users, in a natural but abstract fashion, to support the development of new recommendation algorithms.\n",
        "At its core, a RecSim simulation consists of running the following event loop for some fixed number of sessions (episodes):\n",
        "\n",
        "\n",
        "\n",
        "![RecSim at a glance](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/recsim_at_a_glance.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for episode in [1,...,number_of_episodes]:\n",
        "  user = sample_user()\n",
        "  recommended_slate = null\n",
        "  while session_not_over:\n",
        "    user_response = user_responds_to_recommendation(recommended_slate)\n",
        "    available_documents = sample_documents_from_database()\n",
        "    recommended_slate = agent_step(available_documents, user_response)\n",
        "```\n",
        "\n",
        "The document database (document model), user model, and recommender agent each have various internal components, and we will discuss how to design and implement them in later colabs ([Developing an Environment](RecSim_Developing_an_Environment.ipynb), [Developing an Agent](RecSim_Developing_an_Agent.ipynb)). For now, we will see how to set up one of the ready-made environments that ship with RecSim in order to run a simulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KaWqFNX_E6vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c0cc5b48-9702-47c2-ea38-42e1e5d621a6"
      },
      "source": [
        "# @title Install\n",
        "!pip install --upgrade --no-cache-dir recsim\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting recsim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/ec/ad9e387ac668db49a97b21bbb6ea43bb2faaaa081e73a908370a18f7020d/recsim-0.1.8.tar.gz (60kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 30.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 40kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 51kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from recsim) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: atari-py>=0.2.6 in /usr/local/lib/python3.6/dist-packages (from recsim) (0.2.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dopamine-rl>=2.0.6 (from recsim) (from versions: 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for dopamine-rl>=2.0.6 (from recsim)\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 95.2MB 81kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 55.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tb-nightly 2.1.0a20191120 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "CiB3XD9yy0Ne",
        "colab": {}
      },
      "source": [
        "#@title Importing generics\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ipOB1-sqIOGC"
      },
      "source": [
        "In RecSim, a user model and a document model are packaged together within an OpenAI Gym-style environment. In this tutorial, we will use the \"Interest Evolution\" environment used in [Ie et al.](https://arxiv.org/abs/1905.12767), as well as a full Slate-Q agent also described therein. Both come ready to use with RecSim. We import the environment from recsim.environments. Agents are found in recsim.agents. Finally, we need to import runner_lib from recsim.simulator, which executes the loop outlined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Smj_NRctHZUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "398fd7e7-4a76-45d9-b9b8-bfa22513aed0"
      },
      "source": [
        "#@title Importing RecSim components \n",
        "from recsim.environments import interest_evolution\n",
        "from recsim.agents import full_slate_q_agent\n",
        "from recsim.simulator import runner_lib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-98a27cad816b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrecsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterest_evolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrecsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfull_slate_q_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrecsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrunner_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recsim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq7smdtan_r3"
      },
      "source": [
        "#Creating an Agent\n",
        "\n",
        "Similarly to Dopamine, a RecSim experiment runner (simulator) consumes an environment creation function and an agent creation function. These functions are responsible for setting up the environment/agent based on external parameters. The interest evolution environment already comes with a creation function, so we will limit our attention to the agent.\n",
        "\n",
        "A create_agent function takes a tensorflow session, environment object, a training/eval flag and (optionally) a Tensorflow summary writer, which are passed to the agent for in-agent training statistics in Tensorboard (more on that below). In the case of full Slate-Q, we just need to extract the action and observation spaces from the environment and pass them to the agent constructor. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YdS14uuOn7ZE",
        "colab": {}
      },
      "source": [
        "def create_agent(sess, environment, eval_mode, summary_writer=None):\n",
        "  kwargs = {\n",
        "      'observation_space': environment.observation_space,\n",
        "      'action_space': environment.action_space,\n",
        "      'summary_writer': summary_writer,\n",
        "      'eval_mode': eval_mode,\n",
        "  }\n",
        "  return full_slate_q_agent.FullSlateQAgent(sess, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ekHt96EMx1o"
      },
      "source": [
        "#Training and Evaluating the Agent in a Simulation Loop\n",
        "Before we run the agent, we need to set up a few environment parameters. These are the bare minimum:\n",
        "* *slate_size* sets the size of the set of elements presented to the user;\n",
        "* *num_candidates* specifies the number of documents present in the document database at any given time;\n",
        "* *resample_documents* specifies whether the set of candidates should be resampled between time steps according to the document distribution (more on this in [later notebooks](RecSim_Developing_an_Environment.ipynb)).\n",
        "* finally, we set the random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWXOzVcZMvKk",
        "colab": {}
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "env_config = {\n",
        "  'num_candidates': 10,\n",
        "  'slate_size': 2,\n",
        "  'resample_documents': True,\n",
        "  'seed': seed,\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SY3iY0tdNOGm"
      },
      "source": [
        "Once we've created a dictionary of these, we can run training, specifying additionally the number of training steps, number of iterations and a directory in which to checkpoint the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_b1LKyNONOVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "3bb9ac6b-28a4-4120-8683-5d41795a6ab8"
      },
      "source": [
        "tmp_base_dir = '/tmp/recsim/'\n",
        "runner = runner_lib.TrainRunner(\n",
        "    base_dir=tmp_base_dir,\n",
        "    create_agent_fn=create_agent,\n",
        "    env=interest_evolution.create_environment(env_config),\n",
        "    episode_log_file=\"\",\n",
        "    max_training_steps=50,\n",
        "    num_iterations=10)\n",
        "runner.run_experiment()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7aba9a742ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp_base_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/recsim/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m runner = runner_lib.TrainRunner(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_base_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcreate_agent_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterest_evolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'runner_lib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFgCmk572rc8"
      },
      "source": [
        "After training is finished, we can run a separate simulation to evaluate the agent's performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eVsxaPxfCEQx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "52374cfd-79ed-4754-861c-cb0ceeab2548"
      },
      "source": [
        "  runner = runner_lib.EvalRunner(\n",
        "      base_dir=tmp_base_dir,\n",
        "      create_agent_fn=create_agent,\n",
        "      env=interest_evolution.create_environment(env_config),\n",
        "      max_eval_episodes=5,\n",
        "      test_mode=True)\n",
        "  runner.run_experiment()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-48142c0f07c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m runner = runner_lib.EvalRunner(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_base_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcreate_agent_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterest_evolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'runner_lib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ojxl_ZAX26wh"
      },
      "source": [
        "The cumulative reward across the training episodes will be stored in *base_dir/eval/*. However, RecSim also exports a more detailed set of summaries, including environment specific ones, that can be visualized in a Tensorboard. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "bFfrAEtpDQzD",
        "colab": {}
      },
      "source": [
        "#@title Tensorboard\n",
        "%tensorboard --logdir=/tmp/recsim/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mFLLAj6AjTvx"
      },
      "source": [
        "## References\n",
        "[SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets. IJCAI 2019: 2592-2599](https://arxiv.org/abs/1905.12767)"
      ]
    }
  ]
}